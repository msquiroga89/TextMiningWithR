---
title: 'Cap 3_Analyzing word and document frequency: tf-idf'
author: "Macarena Quiroga"
date: "1/19/2022"
output: html_document
---

Una forma de ver el contenido de un documento es a partir de la frecuencia de las palabras, lo que se puede calcular con el **term frequency (tf)**. Una forma de acercarse a este problema es eliminando las *stop words* (artículos, preposiciones, conjunciones). Otra es mirar la **frecuencia inversa del documento (idf)**, que aumenta el peso de las palabras poco frecuentes y disminuye las de aquellas que son muy frecuentes. Esto se combina con la frecuencia de los términos para calcular el **tf-idf** de un término (las dos cantidades multiplicadas), que indica la frecuencia de un término ajustada a la frecuencia general.

El estadístico *tf-idf* intenta calcular la importancia de un término dentro de un documento. Si bien las cuantificaciones en este sentido es útil para la minería de textos, su base teórica no es del todo avalada por los expertos de la teoría de la información. Se puede definir de la siguiente forma: $tf-idf(term)=ln(\frac{n_{documents}}{n_{documents~containing~term}})$.

# Frecuencia de los términos en las novelas de Jane Austen

```{r}
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>% 
  unnest_tokens(word, text) %>% 
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarise(total = sum(n))
# hace un df de las cantidades totales para hacer el join

book_words <- left_join(book_words, total_words)
```

Ahora, la frecuencia relativa:
```{r}
library(ggplot2)
ggplot(book_words, aes(n/total, fill = book))+
  geom_histogram(show.legend = FALSE)+
  xlim(NA, 0.0009)+
  facet_wrap(~book, ncol = 2, scales = "free_y")
```

# Ley de Zipf

Esta ley indica que la frecuencia con la que aparece una palabra es inversamente proporcional a su rango (esto significa que la segunda palabra más frecuente aparecerá con una frecuencia de 1/2 respecto de la primera, la tercera con una frecuencia de 1/3 respecto de la primera, y así sucesivamente). A partir de la df anterior, se puede graficar fácilmente:

```{r}
freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), #el rango es el orden
         `term frequency` = n/total) %>% 
  ungroup()
```

La ley de Zipf se puede visualizar con el rango en el eje-x y la frecuencia del término en el eje-y, en escalas logarítmicas. Una relación inversamente proporcional va a mostrar una pendiente negativa constante.

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book))+
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE)+
  scale_x_log10()+
  scale_y_log10()
```

Si bien no se cumple la ley con exactitud, se puede pensar que se sigue una ley potencial dividida en tres partes. Se puede ver cuál es el exponente de la ley potencial en la sección media del rango.

```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`)~log10(rank), data = rank_subset)
```

La pendiente está bastante cerca de 1. Vamos a visualizar estos datos ajustados de la ley potencial con los datos de la sección anterior:

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, 
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

Estos resultados se acercan bastante a la ley de Zipf, y las desviaciones son comunes en los estudios lingüísticos. Los corpus suelen contener menos palabras raras que las predichas por una ley potencial.