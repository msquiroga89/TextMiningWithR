---
title: "Cap 4_Relationships between words_n-grams and correlations"
author: "Macarena Quiroga"
date: "2/13/2022"
output: html_document
---

En este capítulo vamos a ver distintos métodos para calcular y visualizar las relaciones *entre* las palabras del dataset. El token que se usa es **ngrams**, que consiste en pares de palabras adyacentes. Agregamos dos paquetes nuevos: `ggraph`, que extiende `ggplot2`, y `widyr`, que calcula correlaciones apareadas y distancias dentro de un df.

## 4.1 Tokenizando por n-gram

La función `unnest_tokens()` también nos sirve para tokenizar con "n-gram" como unidad, lo cual nos permite ver qué tan seguido la palabra X es seguida por la palabra Y. Especificamos `token = "ngrams"` y `n = 2`, para aclarar que buscamos pares de dos palabras consecutivas:

```{r}
library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams <- austen_books() %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

## 4.2 Contar y filtrar n-grams

Ahora podemos contar cuáles son los bigrams más frecuentes:

```{r}
austen_bigrams %>% 
  count(bigram, sort = TRUE)
```

Lo que vemos es que los bigrams más frecuentes son pares de palabras que no nos interesan, artículos y preposiciones. Para mejorar esto, podemos usar la función `separate()`, que divide una columna en varias; vamos a dividir entre "palabra1" y "palabra2", para remover los casos donde alguna de ellas es una stop-word.

```{r}
library(tidyr)
bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% # quedarse con las palabras que no se encuentran en el df de stop_words, en la columna específica de las palabras
  filter(!word2 %in% stop_words$word)

# contamos de nuevo
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE) # acá cuenta combinaciones de palabras
```

Si queremos volver a unificar las dos palabras en un bigram, podemos usar `unite()`:
```{r}
bigrams_united <- bigrams_filtered %>% 
  unite(bigram, word1, word2, sep = " ")
```

Podemos trabajar con trigrams:
```{r}
austen_trigrams <- austen_books() %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>% 
  count(word1, word2, word3, sort = TRUE)
```


## 4.2 Analizando bígramos

Los bígramos nos sirven para hacer análisis exploratorios. Si quisiéramos saber cuáles son las calles más frecuentes, podemos buscar aquel bígramo cuya segunda palabra sea "street":
```{r}
bigrams_filtered %>% 
  filter(word2 == "street") %>% 
  count(book, word1, sort = TRUE)
```

Los bígramos también pueden ser utilizados como unidades para análisis como los realizados en el capítulo 3, a partir del cálculo del tf_idf.
```{r}
bigram_tf_idf <- bigrams_united %>% 
  count(book, bigram) %>% 
  bind_tf_idf(bigram, book, n) %>% 
  arrange(desc(tf_idf))

library(forcats)
library(ggplot2)

bigram_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

El gráfico muestra que los bígramos más usados en cada libro corresponden a los personas más importantes, lo cual indica que el cálculo estuvo bien realizado. Usar bígramos tiene la ventaja de dar más contexto a las palabras más frecuentes, pero tiene la desventaja de que el análisis está más disperso, porque el bígramo más frecuente tiene una frecuencia mucho menor que cualquiera de las dos palabras por separado. Por lo tanto, es un análisis que solo tiene sentido en datasets largos.

En el capítulo 2 hicimos un análisis de sentimiento solamente analizando palabras según su significado positivo o negativo; sin embargo, el contexto de una palabra puede ser tanto o más importante que su presencia. Por ejemplo, las palabras "gusta" y "contento" cuentan como positivas aunque la frase sea "No me gusta y no estoy contento".

Tener los datos organizados en bígramos nos permite ver, por ejemplo, cuántas palabras están emparejadas con la palabra "not":
```{r}
bigrams_separated %>% 
  filter(word1 == "not") %>% 
  count(word1, word2, sort = TRUE)
```

Podemos usar esta información para excluir o bien para invertir los puntajes. Vamos a usar el diccionario AFINN, que da valores numéricos para el sentimiento de cada palabra, ya sean números positivos o negativos. Con eso, podemos examinar las palabras más frecuentes que estuvieron precedidas por "not" y que estaban asociadas con un sentimiento.
```{r}
AFINN <- get_sentiments("afinn")
not_words <- bigrams_separated %>% 
  filter(word1 == "not") %>% 
  inner_join(AFINN, by = c(word2 = "word")) %>% 
  count(word2, value, sort = TRUE)
```

A partir de esto podemos calcular cuántas de estas palabras aportaron a la dirección "opuesta". Para eso, multiplicamos su valor por la cantidad de veces que aparecen.
```{r}
not_words %>% 
  mutate(contribution = n*value) %>% 
  arrange(desc(abs(contribution))) %>% # en valores absolutos porque hay negativos
  head(20) %>%  # solo los primeros 20
  mutate(word2 = reorder(word2, contribution)) %>% 
  ggplot(aes(contribution, word2, fill = contribution > 0))+
  geom_col(show.legend = FALSE)+
  labs(x = "Sentiment value * number of occurrences",
       y = "Words preceded by \"not\"")
```

El gráfico nos muestra que los bígramos "not like" y "not help" generaron que el texto pareciera más positivo de lo que era, y frases como "not afraid" y "not fail" lo hicieron parecer más negativo de lo que era.

"Not" no es la única palabra que puede negativizar las palabras que le siguen. Podemos elegir otras cuatro que cumplen esta misma función. Si quisiéramos, podríamos utilizar la misma estrategia que antes para revertir los puntajes de cada palabra.
```{r}
negation_words <- c("not", "no", "never", "without")
negated_words <- bigrams_separated %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(AFINN, by = c(word2 = "word")) %>% 
  count(word1, word2, value, sort = TRUE)
```

### 4.1.4 Visualizar una red de bígramos con ggraph

Podemos visualizar la relación entre todas las palabras a partir de una red de puntos conectados. Una red se puede crear a partir de un objeto tidy porque tiene tres variables: el nodo **desde** el cual sale una arista, el nodo **hacia** el cual va la arista, y el **peso**, que es el valor numérico asociado con cada arista.

El paquete `igraph` tiene muchas funciones poderosas para manipular y analizar redes. Una forma de crear un objeto igraph a partir de tidy data es la función `graph_from_data_frame()`, que toma un data frame de aristas con las columnas *from*, *to* y los atributos de la arista (en este caso, *n*).
```{r}
library(igraph)
bigram_graph <- bigram_counts %>% 
  filter(n > 20) %>% # para filtrar los bígramos más comunes
  graph_from_data_frame()
```

Para visualizar esta red, podemos usar el paquete `ggraph`, que usa la misma gramática de gráficos que `ggplot2`. Podemos convertir un objeto igraph en un objeto ggraph con la función `ggraph`; luego, agregamos capas de la misma forma que lo haríamos con `ggplot2`. Por ejemplo, en este caso necesitaríamos agregar tres capas: nodos, aristas y texto.
```{r}
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr")+
  geom_edge_link()+
  geom_node_point()+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

Podemos mejorar la estética del gráfico agregando `edge_alpha` a la capa del link para volverlos transparentes según su grado de rareza. Agregamos direccionalidad con una flecha con `grid::arrow()`, incluyendo la opción `end_cap` que le indica a la flecha que termine antes de tocar el nodo. Modificamos las opciones de la capa del nodo para hacerlos más atractivos (más grandes, de color azul). Finalmente, agregamos un theme que es útil para este tipo de redes, `theme_void()`.

```{r}
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr")+
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "lightblue", size = 5)+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
  theme_void()
```

Esta es una visualización del tipo **Markov chain**, donde cada elección de palabra depende solo de la palabra previa. En este caso, un generador al azar que use este modelo podría devolver "dear", después "sir" y después "william/walter/thomas/thomas's", agregando a cada palabra la palabra más común que le sigue. Para poder volver interpretable la visualización, elegimos mostrar solamente las conexiones más frecuentes entre palabras, pero uno podría imaginar un gráfico gigantesco que represente todas las conexiones que se dan en el texto.

### 4.1.5 Visualizar bígramos en otros textos

Podemos utilizar todo el trabajo realizado para limpiar y visualizar los bígramos en el dataset de texto para convertirlo en una función que nos permita realizarlo en otro dataset.

```{r}
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(igraph)
library(ggraph)

count_bigrams <- function(dataset){
  dataset %>% 
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
    separate(bigram, c("word1", "word2"), sep = " ") %>% 
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>% 
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams){
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>% 
    graph_from_data_frame() %>% 
    ggraph(layout = "fr")+
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a)+
    geom_node_point(color = "lightblue", size = 5)+
    geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
    theme_void()
}
```

Esto nos permite visualizar bígramos en otras obras, como en la versión King James de la Biblia:
```{r}
library(gutenbergr)
kjv <- gutenbergr::gutenberg_download(10)

library(stringr)
kjv_bigrams <- kjv %>% 
  count_bigrams()

kjv_bigrams %>% 
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>% 
  visualize_bigrams()
```

## 4.2 Contar y correlacionar pares de palabras con el paquete `widyr`

Tokenizar a partir de n-grams es una buena forma de explorar pares de palabras adyacentes. Sin embargo, también podemos estar interesados en palabras que tienden a co-ocurrir dentro de determinados documentos y capítulos, aunque no aparezcan una al lado de la otra.

Tidy data es una estructura útil para comparar entre variables o agrupaciones de filas, pero puede ser difícil comparar entre filas: por ejemplo, para contar la cantidad de veces que dos palabras aparece dentro del mismo capítulo, o para ver qué tan correlacionadas están. La mayoría de las operaciones para encontrar conteos emparejados o correlaciones necesitan convertir los datos en una matriz ancha [wide] primero.

Vamos a examinar algunas formas en las que se pueden convertir textos tidy en matrices anchas en el capítulo 5, pero en este caso no es necesario. El paquete `widyr` permite realizar operaciones como computar conteos y correlaciones de forma sencilla, simplificando el patrón de "ensanchar los datos, realizar la operación, volver a limpiar los datos". Nos enfocaremos en una serie de funciones que realizan comparaciones emparejadas entre grupos de observaciones (por ejemplo, entre documentos o entre secciones de un mismo texto).

### 4.2.1 Contar y correlacionar entre secciones

Tomemos el libro "Pride and prejudice" dividido en secciones de 10 líneas, como hicimos para el análisis de sentimientos en el Capítulo 2. Podemos estar interesados en qué palabras tienden a aparecer dentro de la misma sección.

```{r}
austen_section_books <- austen_books() %>% 
  filter(book == "Pride & Prejudice") %>% 
  mutate(section = row_number() %/% 10) %>% # %/% significa división de integrales (división completa sin excedentes), por lo que entiendo excluye las primeras 10 líneas 
  filter(section > 0) %>% 
  unnest_tokens(word, text) %>% 
  filter(!word %in% stop_words$word)
```

Una función útil del paquete `widyr` es `pairwise_count()`. El prefijo `pairwise_` indica que el resultado es una fila por cada par de palabras en la variable `word`. Esto nos permite contar pares comunes de palabras co-apareciendo dentro de la misma sección.
```{r}
library(widyr)

# conteo de palabras que co-aparecen dentro de las secciones
word_pairs <- austen_section_books %>% 
  pairwise_count(word, section, sort = TRUE)
```

Por lo tanto, mientras que el input tenía una fila para una sección del documento y una palabra, el output tiene una fila por cada par de palabras. Esto también es un formato tidy, pero con una estructura distinta que nos permitirá responder nuevas preguntas.

Por ejemplo, podemos ver que los pares de palabras más comunes dentro de una sección son "Elizabeth" y "Darcy" (los dos protagonistas). Podemos fácilmente encontrar las palabras que co-ocurren más frecuentemente con Darcy:
```{r}
word_pairs %>% 
  filter(item1 == "darcy")
```
