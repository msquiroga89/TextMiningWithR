---
title: "Sentiment analysis with tidy data"
author: "Macarena Quiroga"
date: "12/26/2021"
output: html_document
---

Este capítulo es sobre el análisis de opiniones o de sentimiento, y parte de este flujo de trabajo: 

![](https://www.tidytextmining.com/images/tmwr_0201.png)

Para esto, vamos a trabajar con el paquete `sentiments`. Se usan distintos tipos de diccionarios de sentimientos, los cuales asignan valoraciones positivas o negativas a las palabras, además de posibles emociones asociadas a ellas. Algunos utilizan puntajes. Las palabras consideradas *neutrales* no aparecen en estos diccionarios. No se toman en cuenta los determinantes negadores (por ejemplo, "no divertido"); el texto que se va a utilizar más adelante no tiene grandes segmentos de negaciones ni de sarcasmo. Una última cosa a tener en cuenta es que si se analiza un texto de muchos párrafos, es muy probable que la suma total dé cero, por lo cual es mejor analizar oraciones o párrafos.

La función `get_sentiments()` recupera los diccionarios posibles. 
```{r}
library(tidytext)
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc") 
```
Cuando se usan estos diccionarios de sentimientos, el sentimiento total de un texto se calcula sumando los puntajes individuales de las distintas palabras que lo contienen. 

## Análisis de sentimiento con `inner_join()`

Para analizar la emocionalidad de uno de los libros, primero filtramos del diccionario las palabras relacionadas con la alegría [joy]. Luego seleccionamos solamente las palabras de un libro ("Emma") y realizamos el análisis de sentimientos con un `inner_join()`.
```{r}
library(tidyverse)
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

Vemos acá una lista de las palabras felices más frecuentes. También podemos ver cómo cambia la emocionalidad a lo largo del libro: se cuentan cuántas palabras positivas y negativas hay en cada sección del libro. El operador `%/%` funciona como `floor(x/y)` y entiendo que sirve para identificar secciones de determinada cantidad (en este caso, 80 líneas), con la función `index`. Luego pivotea las columnas para tener los valores positivos y los negativos en columnas separadas y finalmente calcula un valor total:

```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  # values_fill indica con qué hay que rellenar una columna si no tiene datos
  mutate(sentiment = positive - negative)
```

Una vez que tenemos esto, podemos graficar la evolución:
```{r}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

# Comparar los diccionarios

Al haber distintos diccionarios, puede ser interesante compararlos para ver cuál se ajusta mejor a los objetivos del trabajo. Vamos a ver cómo cambia el arco narrativo de `Pride and Prejudice` con los tres diccionarios. Seleccionamos las palabras de ese libro y luego usamos `inner_join()` para el cálculo. Como usan distintos métodos (AFINN usa un puntaje entre -5 y 5, mientras que los otros clasifican las palabras entre positivas y negativas de forma binaria), habrá que usar patrones de código distintos.

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  # un valor de sentimiento por cada bloque de 80 líneas
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  # el puntaje es el conteo de las palabras positivas y las negativas
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

Ahora que ya tenemos este análisis, podemos graficarlos en conjuntos:

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
Si bien vemos un patrón similar en los tres diccionarios, los valores absolutos son distintos. ¿Por qué los resultados del NRC son más altos que los demás? Esto se puede analizar mirando el ratio de positividad en los diccionarios:

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)
```
Ambos tienen más palabras negativas que positivas, pero el ratio es distinto. Esto va a contribuir al resultado de los análisis que se hagan.

# Palabras positivas y negativas más comunes

Una de las ventajas de tener las palabras clasificadas en positivas y negativas es que permite saber el conteo de las palabras que aportan a cada una de las cantidades.

```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% # solo los primeros diez
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

Lo que vemos es que la palabra "miss" podría referirse a "señorita" y no a "extrañar", así que en el caso de necesitarlo podríamos armar una base de stopwords propia y agregarla:

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

# Nubes de palabras

Podemos mirar las palabras más comunes en los libros de Jane Austen, con el paquete `wordcloud`, que se basa en los gráficos de Rbase.
```{r}
library(wordcloud)

tidy_books %>%
  anti_join(custom_stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
La función `comparison.cloud()` te permite ordenar las palabras según uno de los criterios. Para eso hay que convertir la tabla a una matriz con `reshape2::acast()`. El primer argumento es la fórmula según la cual se ordenan los elementos (en este caso, se ordenan las palabras según el tipo de sentimiento), luego `value.var` indica el nombre de la columna donde se encuentran los valores y `fill` es el valor con el cual se llenarán los NA. El tamaño de las palabras es relativo a la frecuencia dentro de su tipo de sentimiento, por lo cual no son comparables entre sí.
```{r}
library(reshape2)
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

# Mirando a unidades más allá de las palabras

Algunos análisis intentan ir más allá de los *unigrams* (palabras) para poder entender oraciones complejas, como "No estoy teniendo un buen día", que es una oración triste gracias a la negación. Paquetes como `coreNLP`, `cleanNLP`, y `sentimentr` hacen este análisis a partir de oraciones. Para esto, vamos a tokenizar el texto con oraciones, creando una nueva columna; como la unidad default para tokenizar es la palabra, en este caso hay que especificarlo con el argumento `token = "sentences"`:

```{r}
p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentences, text, token = "sentences")

p_and_p_sentences$sentences[2]
```
Este ejemplo demuestra que pueden aparecer problemas cuando el texto tiene una codificación UTF-8, sobre todo en secciones dialogadas; trabaja mejor cuando está codificado como ASCII. Una opción podría ser usar la función `iconv(text, to = 'latin1')` antes de tokenizar.

```{r}
p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  mutate(text = iconv(text, to = 'latin1')) %>% 
  unnest_tokens(sentences, text, token = "sentences")

p_and_p_sentences$sentences[50]
```

Acá lo probé, pero no parece haber cambiado mucho (o tal vez yo no termino de entender cuál es el problema y/o cuál debería ser la solución). En fin, sigamos.

Otras unidades para tokenizar pueden ser por ejemplo los capítulos. Para esto podemos usar una regex, para tener un df por cada capítulo:

```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```
Con eso tenemos una lista de la cantidad de capítulos que tiene cada libro; cada fila del df `austen_chapters` corresponde a un capítulo. A partir de esto podemos preguntarnos cuáles son los capítulos más tristes. Para eso, primero extraemos todas las palabras negativas del diccionario Bing, luego extraemos las palabras totales de cada capítulo para poder normalizar las extensiones, y finalmente obtenemos la cantidad de palabras negativas de cada capítulo divididas por el total de palabras por capítulo.

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>% 
  group_by(book, chapter) %>% 
  summarise(words = n())

tidy_books %>% 
  semi_join(bingnegative) %>% # solo las palabras que aparecen en ambos df
  group_by(book, chapter) %>% 
  summarise(negativewords = n()) %>%  # cuenta el total de las palabras negativas en cada capítulo
  left_join(wordcounts, by = c("book", "chapter")) %>% # acá agrega a las líneas de esta df las correspondientes cantidades ya calculadas en el df wordcounts
  mutate(ratio = negativewords/words) %>% 
  filter(chapter != 0) %>% 
  slice_max(ratio, n = 1) %>%  # no termino de entender
  ungroup()
```

