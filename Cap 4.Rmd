---
title: "Cap 4_Relationships between words_n-grams and correlations"
author: "Macarena Quiroga"
date: "2/13/2022"
output: html_document
---

En este capítulo vamos a ver distintos métodos para calcular y visualizar las relaciones *entre* las palabras del dataset. El token que se usa es **ngrams**, que consiste en pares de palabras adyacentes. Agregamos dos paquetes nuevos: `ggraph`, que extiende `ggplot2`, y `widyr`, que calcula correlaciones apareadas y distancias dentro de un df.

## 4.1 Tokenizando por n-gram

La función `unnest_tokens()` también nos sirve para tokenizar con "n-gram" como unidad, lo cual nos permite ver qué tan seguido la palabra X es seguida por la palabra Y. Especificamos `token = "ngrams"` y `n = 2`, para aclarar que buscamos pares de dos palabras consecutivas:

```{r}
library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams <- austen_books() %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

## 4.2 Contar y filtrar n-grams

Ahora podemos contar cuáles son los bigrams más frecuentes:

```{r}
austen_bigrams %>% 
  count(bigram, sort = TRUE)
```

Lo que vemos es que los bigrams más frecuentes son pares de palabras que no nos interesan, artículos y preposiciones. Para mejorar esto, podemos usar la función `separate()`, que divide una columna en varias; vamos a dividir entre "palabra1" y "palabra2", para remover los casos donde alguna de ellas es una stop-word.

```{r}
library(tidyr)
bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% # quedarse con las palabras que no se encuentran en el df de stop_words, en la columna específica de las palabras
  filter(!word2 %in% stop_words$word)

# contamos de nuevo
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE) # acá cuenta combinaciones de palabras
```

